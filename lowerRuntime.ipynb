{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae0de5-e9c3-4f2a-b778-c657e01638b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk # natura|l language tool kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c1c98-5f24-46ba-a399-ed9023049a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialFrame = pd.read_csv('train.tsv', delimiter = '\\t');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e54af4-c210-4760-b1f5-00e6a57b157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d97fc68-19de-44b8-b7a5-85c83d4c426d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cleans the initial frame\n",
    "def cleanInitialFrame(df):\n",
    "    cleanList = [] # list to grow\n",
    "    currentSentence = 0 # tracks current sentence\n",
    "    sentenceIDs = {0} \n",
    "    # Iterate row by row\n",
    "    for index, row in df.iterrows():\n",
    "        # If it's the first element, add to list\n",
    "        if (row['SentenceId'] == currentSentence):\n",
    "            continue\n",
    "        else:\n",
    "            cleanList.append([row['PhraseId'], row['SentenceId'], row['Phrase'], row['Sentiment']]);\n",
    "            currentSentence = row['SentenceId']\n",
    "    \n",
    "    # Return a clean frame\n",
    "    return pd.DataFrame(cleanList, columns = ['PhraseId', 'SentenceId', 'Phrase', 'Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073371f0-8f01-4627-b764-5c9c57a17841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleanInitialFrame(initialFrame)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79094ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowerAllPhrases(df):\n",
    "    phrases_list = list(df['Phrase'])\n",
    "\n",
    "    for i in range(len(phrases_list)):\n",
    "        phrases_list[i] = phrases_list[i].lower()\n",
    "    count = 0;\n",
    "    for index, row in df.iterrows():\n",
    "        df.at[index,'Phrase'] = phrases_list[count]\n",
    "        count += 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowerAllPhrases(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb27c8-3e41-411a-adb8-adfbfedbf29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-ascii characters using str.replace()\n",
    "def asciiClean(df):\n",
    "    # iterate row by row\n",
    "    for index, row in df.iterrows():\n",
    "        old_str = row['Phrase']\n",
    "        new_str = (old_str.encode('ascii','ignore')).decode()\n",
    "        df.at[index, 'Phrase'] = new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3544c935-4b22-4694-bcfa-1cea5e307ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "asciiClean(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863aacad-9db6-4885-892e-583af48d1b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d591f9e-4d34-4e4e-ad0a-86b5fa857531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def removeSpaces(df):\n",
    "    for index, row in df.iterrows():\n",
    "        # df['Phrase'] = df['Phrase'].replace([row['Phrase']], re.sub(r'\\s+\\'', \"'\", row['Phrase']))\n",
    "        df.at[index,'Phrase'] = re.sub(r'\\s+\\'', \"'\", row['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e9877-ff45-4f2c-9f27-7b7be16d7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "removeSpaces(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25e872-620e-498b-a3c0-91f9b5131f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f1351-a2cf-47e2-aa2c-6b317654e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "contractions.add('n\\'t', 'not')\n",
    "def expandContractions(df):\n",
    "    for index, row in df.iterrows():\n",
    "        phrase = []\n",
    "        for i in row['Phrase'].split():\n",
    "            phrase.append(contractions.fix(i))\n",
    "        string_version = ' '.join(phrase)\n",
    "        df.at[index, 'Phrase'] = string_version            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8090cd-7829-4229-8fa5-3acebc9ab3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expandContractions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec2e70-f456-4161-9f02-4365f68f4e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666e402-ba3c-4df3-8a7f-1a0ab7cf14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(df):\n",
    "    \n",
    "    phrases_list = list(df['Phrase'])\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for i in range(len(phrases_list)):\n",
    "        word_tokens = word_tokenize(phrases_list[i])\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        filtered_sentence = []\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        phrases_list[i] = filtered_sentence\n",
    "    \n",
    "    for i in range(len(phrases_list)):\n",
    "        phrases_list[i] = TreebankWordDetokenizer().detokenize(phrases_list[i])\n",
    "    \n",
    "    count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        # df['Phrase'] = df['Phrase'].replace([row['Phrase']], phrases_list[count])\n",
    "        df.at[index, 'Phrase'] = phrases_list[count]\n",
    "        count += 1\n",
    "\n",
    "    return df      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1abbf5a-b4b1-4ee3-b756-3ebd36ccc3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = removeStopWords(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00363f84-4519-44ef-ae5a-9af2e9f53d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# * preprocessing\n",
    "#     - replace all grammar with spaces\n",
    "#     - lemmatize words\n",
    "# * reduce to multi-dimensional vector\n",
    "#     - We have options including: Bag of words\n",
    "#     - BERT\n",
    "#     - TF-IDF\n",
    "# * classification\n",
    "#     - try a bunch of classifiers\n",
    "# * graphing\n",
    "#     - graph the outputs of our classifiers\n",
    "# * presentation\n",
    "#     - create a presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42f222-d3e1-4802-9da8-d1f2597a8b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0d713-b74e-44ab-90d1-5780bfe2843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selene(object):\n",
    "    \n",
    "    def __init__(self, dataFrame):\n",
    "        \n",
    "        self.df = dataFrame\n",
    "        self.unique = []\n",
    "        self.apperances = []\n",
    "        self.wordsDict = {}\n",
    "        self.wordSentOcc = {}\n",
    "        self.wordSentVal = {}\n",
    "    # Finds unique words in the list of phrases and puts into list\n",
    "    def setUniqueWords(self):\n",
    "        \n",
    "        phrases = list(self.df['Phrase'])\n",
    "        \n",
    "        for i in range(len(phrases)):\n",
    "            word_tokens = word_tokenize(phrases[i])\n",
    "            for w in word_tokens:\n",
    "                if w.isalpha():\n",
    "                    self.wordsDict[w] = 0\n",
    "                # This logic needs to be removed\n",
    "                # for l in w:\n",
    "                #     if (l.isalpha()):\n",
    "                #         flag = 1\n",
    "                # if (w not in self.unique) and (len(w) > 2) and (not flag):\n",
    "                #     self.wordsDict[w] = 0\n",
    "                    \n",
    "        # print(\"There are {} unique words in this data.\".format(len(self.unique)))\n",
    "    # Finds frequency of a word in the bag of words and puts into list\n",
    "    def setApperances(self):\n",
    "        \n",
    "        phrases = list(self.df['Phrase'])\n",
    "        \n",
    "#         for word in self.unique:\n",
    "#             currWord = word\n",
    "#             counter = 0\n",
    "#             for i in range(len(phrases)):\n",
    "#                 counter += phrases[i].count(currWord)\n",
    "#             self.wordsDict[word] += 1\n",
    "        \n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            sentence = row['Phrase']\n",
    "            for word in sentence:\n",
    "                self.wordsDict[word] += 1\n",
    "            \n",
    "#         max_app = max(self.apperances)\n",
    "#         max_ind = self.apperances.index(max_app)\n",
    "        \n",
    "#         print(\"The word with the most apperances is {} with {} \"\n",
    "#               \"apperances.\".format(self.unique[max_ind], max_app))\n",
    "    # Converts the two lists into a dictionary\n",
    "#     def setWordsDict(self):\n",
    "        \n",
    "#         index = 0\n",
    "#         for word in self.unique:\n",
    "#             # Declare dictionary: (frequency, 0 frequency, 1 frequency, 2 frequency, 3 frequency, 4 frequency)\n",
    "#             self.wordsDict[word] = (self.apperances[index], 0, 0, 0, 0, 0)\n",
    "#             index += 1\n",
    "            \n",
    "#         print(\"Here is all the unique words and their number of occurances:\")\n",
    "        \n",
    "#         for key, value in self.wordsDict.items():\n",
    "#             print(key, \": \", value)\n",
    "            \n",
    "    # Runs things\n",
    "    def extractUniqueWords(self):\n",
    "        \n",
    "        self.setUniqueWords()\n",
    "        self.setApperances()\n",
    "        # self.setWordsDict()\n",
    "    # Finds frequencies of the sentiment per word\n",
    "    def findWordSentOccurances(self):\n",
    "        \n",
    "#         phrases = list(self.df['Phrase'])\n",
    "        \n",
    "#         for word in self.unique:\n",
    "#             currWord = word\n",
    "#             self.wordSentOcc[word] = {'0': 0, '1': 0, '2': 0, '3': 0, '4':0}\n",
    "#             occ0 = 0; occ1 = 0; occ2 = 0; occ3 = 0; occ4 = 0\n",
    "#             for i in range(len(phrases)):\n",
    "#                 occurances = phrases[i].count(currWord)\n",
    "#                 senti = self.df.iloc[i]['Sentiment']\n",
    "#                 if senti == 0:\n",
    "#                     occ0 += occurances\n",
    "#                     self.wordSentOcc[word]['0'] = occ0\n",
    "#                 if senti == 1:\n",
    "#                     occ1 += occurances\n",
    "#                     self.wordSentOcc[word]['1'] = occ1\n",
    "#                 if senti == 2:\n",
    "#                     occ2 += occurances\n",
    "#                     self.wordSentOcc[word]['2'] = occ2\n",
    "#                 if senti == 3:\n",
    "#                     occ3 += occurances\n",
    "#                     self.wordSentOcc[word]['3'] = occ3\n",
    "#                 if senti == 4:\n",
    "#                     occ4 += occurances\n",
    "#                     self.wordSentOcc[word]['4'] = occ4\n",
    "                    \n",
    "        for index, rows in df.iterrows():\n",
    "            sentiment = int(row['Sentiment'])\n",
    "            for word in row['Phrase']:\n",
    "                if word.isalpha():\n",
    "                    self.wordsDict[word][sentiment + 1] += 1\n",
    "                \n",
    "        \n",
    "        print(\"All the times each word has appeared with a certain sentiment has been set.\"\n",
    "              \" The results are the following:\")\n",
    "        for key, value in self.wordSentOcc.items():\n",
    "            print(key, \": \", value)\n",
    "    # Finds probabilities for a word's sentiment\n",
    "    def findProbabilities(self, key):\n",
    "        \n",
    "        prob0 = self.wordSentOcc[key][0 + 1] / self.wordsDict[key]\n",
    "        prob1 = self.wordSentOcc[key][1 + 1] / self.wordsDict[key]\n",
    "        prob2 = self.wordSentOcc[key][2 + 1] / self.wordsDict[key]\n",
    "        prob3 = self.wordSentOcc[key][3 + 1] / self.wordsDict[key]\n",
    "        prob4 = self.wordSentOcc[key][4 + 1] / self.wordsDict[key]\n",
    "        return prob0, prob1, prob2, prob3, prob4\n",
    "    # Finds the average sentiment of a word\n",
    "    def findWordSentVal(self):\n",
    "        \n",
    "        for key, value in self.wordSentOcc.items():\n",
    "            sentVal = (((value[0 + 1] * 0) + (value[1 + 1] * 1) + (value[2 + 1] * 2)\n",
    "                    + (value[3 + 1] * 3) + (value[4 + 1] * 4)) / self.wordsDict[key])\n",
    "            prob0, prob1, prob2, prob3, prob4 = self.findProbabilities(key)\n",
    "            self.wordSentVal[key] = {'Avg Value': sentVal,'Probability of 0': prob0,\n",
    "                                     'Probability of 1': prob1, 'Probability of 2': prob2, \n",
    "                                     'Probabilty of 3': prob3, 'Probability of 4': prob4}\n",
    "            \n",
    "        print(\"Here is the average value of the sentiment and probabilities of each word in the training data:\")\n",
    "        for key, value in self.wordSentVal.items():\n",
    "            print(key, \": \", value)\n",
    "            \n",
    "    def getSentiment(self, testDataFrame, weightPercent = .1, minimumOccurances = 3):\n",
    "        pass\n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c188e5-b06e-45c4-b591-aab0a1df1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Selene(df)\n",
    "test.extractUniqueWords()\n",
    "test.findWordSentOccurances()\n",
    "test.findWordSentVal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2786de-3131-429f-97dd-24b740c23f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
