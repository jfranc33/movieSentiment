{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf17c72-6546-4291-8107-bf7e326fa4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk # natural language tool kit\n",
    "import contractions\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import pos_tag, word_tokenize   \n",
    "import random\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe89911-008b-414b-83d0-50dcde87dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.tsv', sep='\\t')\n",
    "test = pd.read_csv('./test 2.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2008b391-f520-4e06-95cb-d5ee38ca46ec",
   "metadata": {},
   "source": [
    "The sentiment labels are:\n",
    "\n",
    "0 - negative\n",
    "\n",
    "1 - somewhat negative\n",
    "\n",
    "2 - neutral\n",
    "\n",
    "3 - somewhat positive\n",
    "\n",
    "4 - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc254b4-3215-4a96-881d-234b8568488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove grammar, tokenize, and lemmatize the sentences\n",
    "def clean_sentences(df):\n",
    "    reviews = []\n",
    "\n",
    "    for sent in tqdm(df['Phrase']):\n",
    "        \n",
    "        # remove non-alphabetical characters\n",
    "        text = re.sub('[^a-zA-Z]', ' ', sent)\n",
    "        \n",
    "        # tokenize sentence\n",
    "        words = word_tokenize(text.lower())\n",
    "        \n",
    "        # remove stop words:\n",
    "        new_words = [char for char in words if char.lower() not in stopwords.words('english')]\n",
    "        \n",
    "        # lemmatizing each word to its lemma\n",
    "        lem = WordNetLemmatizer()\n",
    "        lem_words = [lem.lemmatize(i) for i in new_words]\n",
    "    \n",
    "        reviews.append(lem_words)\n",
    "\n",
    "    return(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c955477-38da-4280-a5c0-bff6cae4d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sen = clean_sentences(train)\n",
    "test_sen = clean_sentences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edebc2b-6501-41b6-b831-f200879c3ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train['Phrase']), len(train_sen))\n",
    "print(len(test['Phrase']), len(test_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9795da-92cc-4b7b-9bbb-4431270c8e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=train.Sentiment.values\n",
    "y_target=to_categorical(target)\n",
    "num_classes=y_target.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1dc1be-5c75-4c45-b3c4-2cb5719730a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(train_sen,target,\n",
    "                                             test_size=0.2,stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f9736-7017-4cea-961c-ccf28fc6ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set()\n",
    "len_max = 0\n",
    "\n",
    "for sent in tqdm(X_train):\n",
    "    \n",
    "    unique_words.update(sent)\n",
    "    \n",
    "    if(len_max<len(sent)):\n",
    "        len_max = len(sent)\n",
    "        \n",
    "print(len(list(unique_words)))\n",
    "print(len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6220598-452e-43a2-b4e8-d4299aafa298",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(list(unique_words))\n",
    "embedding_dim = 300\n",
    "max_length = len_max\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefccf5-7277-496f-9bf1-6a73c29028a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize stuff\n",
    "tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(test_sen)\n",
    "\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=len_max)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=len_max)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=len_max)\n",
    "\n",
    "print(X_train.shape,X_val.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c2d51-ba41-4c0b-8b79-2335116d8772",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944411c-29e0-46a2-91d7-f0d71e85bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='saga', max_iter = 200, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf790f-23f5-4e4e-86fa-ec0b38be5f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)\n",
    "print(model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b9a38-ef25-4dab-a58f-08df3c17856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6761c9-31a4-4f8a-9f85-95156cbac78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_pred, y_val), display_labels=np.unique(y_val))\n",
    "disp.plot(cmap='Blues') \n",
    "plt.grid(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
